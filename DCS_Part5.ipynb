{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ec240c",
   "metadata": {},
   "source": [
    "### Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ec458",
   "metadata": {},
   "source": [
    "#### SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb28027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL SERVER CONNECTION USING SPARK\n",
    "jdbcHostname = 'your_sql_server_hostname'\n",
    "jdbcPort = 'your_port_number'\n",
    "jdbcUsername = 'sqlserver_username'\n",
    "jdbcPassword = 'sqlserver_password'\n",
    "jdbcDatabase = 'database_name'\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4};\".format(jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword)\n",
    "\n",
    "sqlserver_connection = {\n",
    "    \"user\" : jdbcUsername,\n",
    "    \"password\": jdbcPassword,\n",
    "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"fetchsize\": '50000'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d4628",
   "metadata": {},
   "source": [
    "#### Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852f9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNOWFLAKE CONNECTION USING SPARK\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set options below\n",
    "snowflake_connection = {\n",
    "    \"sfURL\" : 'your_snowflake_url.snowflakecomputing.com',\n",
    "    \"sfUser\" : 'snowflake_username',\n",
    "    \"sfPassword\" : 'snowflake_password',\n",
    "    \"sfDatabase\" : 'database_name',\n",
    "    \"sfSchema\" : 'schema_name',\n",
    "    \"sfWarehouse\" : 'snowflake_warehouse'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec77597",
   "metadata": {},
   "source": [
    "### Table Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfb0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableList = [\n",
    "    'table1'\n",
    "    ,'table2'\n",
    "    ,'table3'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a650e3",
   "metadata": {},
   "source": [
    "### Reading record count from databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba752bf",
   "metadata": {},
   "source": [
    "#### SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'dbo'\n",
    "    \n",
    "for table in tableList:\n",
    "    try:\n",
    "        sql1 = f\"(SELECT COUNT(*) as SQL_RECORD_COUNT, '{table}' as TABLE_NAME, GETDATE() as SQL_SYSDATE FROM {schema}.{table}) a\"\n",
    "        df1 = spark.read.jdbc(url=jdbcUrl, table = sql, properties=sqlserver_connection)\n",
    "        df1.write.mode(\"append\").option(\"header\",True).parquet(\"sqlserver_parquet\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "            \n",
    "df1_merged = spark.read.parquet(\"sqlserver_parquet\")\n",
    "df1_merged.show()\n",
    "\n",
    "# OPTIONAL STEP - write as a CSV locally\n",
    "df1_merged.orderBy(\"TABLE_NAME\").coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\",True).save(\"SQLSERVER_COUNTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb1a1",
   "metadata": {},
   "source": [
    "####  Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d088e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'test'\n",
    "schema = 'public'\n",
    "    \n",
    "for table in tableList:\n",
    "    try:\n",
    "        sql2 = f\"(SELECT COUNT(*) as SNOW_RECORD_COUNT, '{table}' as TABLE_NAME, CURRENT_TIMESTAMP() as SNOW_SYSDATE FROM {database}.{schema}.{table}) a\"\n",
    "        df2 = spark.read.format(\"snowflake\").options(**sf_options).option(\"query\",sql).load()\n",
    "        df2.write.mode(\"append\").option(\"header\",True).parquet(\"snowflake_parquet\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "            \n",
    "df2_merged = spark.read.parquet(\"snowflake_parquet\")\n",
    "df2_merged.show()\n",
    "\n",
    "# OPTIONAL STEP - write as a CSV locally\n",
    "df2_merged.orderBy(\"TABLE_NAME\").coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\",True).save(\"SNOWFLAKE_COUNTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78549c",
   "metadata": {},
   "source": [
    "### FINAL RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f671934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.join(df2,['TABLE_NAME'],\"FULL\")\n",
    "df3 = df3.withColumn(\"DIFF\",col('SNOW_RECORD_COUNT')-col('SQL_RECORD_COUNT'))\n",
    "df3 = df3.withColumn(\"PCT_DIFF\",((col('SNOW_RECORD_COUNT')-col('SQL_RECORD_COUNT'))/col('SQL_RECORD_COUNT'))*100)\n",
    "df3 = df3.select(\"TABLE_NAME\",\"SQL_RECORD_COUNT\",\"SNOW_RECORD_COUNT\",\"DIFF\",\"PCT_DIFF\")\n",
    "df3.show()\n",
    "\n",
    "# OPTIONAL STEP - write as a CSV locally\n",
    "df3.orderBy(\"TABLE_NAME\").coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\",True).save(\"FINAL_RESULT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
